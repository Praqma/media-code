# Video log on Kubernetes service discovery

In this folder you will find example on creating objects in Kubernetes and learn what Service Discovery is. You will use selectors for both Replicaset, Services and PVC objects. You will also try and create a rogue pod with the same labe as our replicaset, and see how the replicaset behaves. Also, you will scale the replicaset and see how the endpoints for the Service will include the new pods automatically.

# Service discovery in Replicaset
Replicaset are used to keep a specific number of identical pods running. When specifying a replicaSet, you can use a selector to select which pods to manage. Examine the files `./replicaset/multitool.yaml`

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: multitool
  labels:
    app: multitool
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: multitool
        image: praqma/network-multitool
```

As we can see the replicaSet has a selector to match pods with the label key `tier` and value `frontend` and in the template it's creating pods with this label as well. This way all pods created by this replicaSet will be controlled by it. We specify 3 replicas, so we would expect 3 pods created by this replicaSet.

Apply the replica set
```
kubectl apply -f multitool.yaml
```

To verify, we can use our own selector when using kubectl to see that we get 3 pods with the label
```
kubectl get pods -l tier=frontend

NAME              READY   STATUS    RESTARTS   AGE
multitool-82c25   1/1     Running   0          14s
multitool-glhjz   1/1     Running   0          14s
multitool-jlgk6   1/1     Running   0          14s
```

# Service discovery in Services
Lets create a service and base it on our multitool pods maching our label. Examine the file `./service/multitool.yaml`

```
apiVersion: v1
kind: Service
metadata:
  name: multitool
  labels:
    tier: frontend
spec:
  selector:
    tier: frontend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
```

The service itself has a label, but it also has a selector to choose and match pods with a specified label. This label has the key `tier` and value `frontend` just like our pods. So lets apply it and see that endpoints we get.

```
kubectl apply -f multitool.yaml
```

Check that the service was created by selecting it with its label
```
kubectl get services -l tier=frontend

NAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
multitool   ClusterIP   10.43.143.184   <none>        80/TCP    5s
```

now check the endpoints generated by the Service
```
kubectl get endpoints -l tier=frontend

NAME        ENDPOINTS                                      AGE
multitool   10.42.0.187:80,10.42.1.142:80,10.42.2.254:80   19s
```
So again the system uses service discovery to find the objects it has an relationship with. 

# Scaling the Replicaset and check endpoints
So, what happens when we scale our replicaset? Do the service get more endpoints?
Let's try.

```
kubectl scale replicaset multitool --replicas 5
```

Let's check created pods
```
kubectl get pods -l tier=frontend

NAME              READY   STATUS    RESTARTS   AGE
multitool-82c25   1/1     Running   0          91s
multitool-bshs2   1/1     Running   0          14s
multitool-glhjz   1/1     Running   0          91s
multitool-jlgk6   1/1     Running   0          91s
multitool-ss5c8   1/1     Running   0          14s
```

Let's check endpoints
```
kubectl get endpoints -l tier=frontend

NAME        ENDPOINTS                                                  AGE
multitool   10.42.0.187:80,10.42.1.142:80,10.42.1.143:80 + 2 more...   88s
```
We see that we now have 5 endpoints (3 listed and + 2 more).

Lets scale it back to three replicas
```
kubectl scale replicaset multitool --replicas 3
```

So building relationsships between objects with labels and selectors is really powerfull. Let's see if we can cheat the system


# Trying to hijack a Replicaset and service with a rogue pod
But, what if we try and create a rogue pod with the same label? Would it be controlled by the replicaSet? Would it get to live? We did specify a replica of 3, and with an additional pod, that would give us 4 which is not the desired state specified in the replicaSet.

Examine the pod manifest in `./pod/multitool.yaml`

```
apiVersion: v1
kind: Pod
metadata:
  name: multitool-rogue
  labels:
    tier: frontend
spec:
  containers:
    - name: multitool-rogue
      image: praqma/network-multitool
      ports:
        - name: web
          containerPort: 80
          protocol: TCP
```

We can see that this pod will be created with a different name, but the same label. Try and apply it, and see what happens

```
kubectl apply -f multitool.yaml
```

Wait a bit, and check for running pods
```
kubectl get pods -l tier=frontend

NAME              READY   STATUS    RESTARTS   AGE
multitool-82c25   1/1     Running   0          10m
multitool-glhjz   1/1     Running   0          10m
multitool-jlgk6   1/1     Running   0          10m
```

How many do you see?

Check the replicaset
```
kubectl describe replicaset multitool

Events:
  Type    Reason            Age               From                   Message
  ----    ------            ----              ----                   -------
  Normal  SuccessfulCreate  10m               replicaset-controller  Created pod: multitool-jlgk6
  Normal  SuccessfulCreate  10m               replicaset-controller  Created pod: multitool-glhjz
  Normal  SuccessfulCreate  10m               replicaset-controller  Created pod: multitool-82c25
  Normal  SuccessfulCreate  9m31s             replicaset-controller  Created pod: multitool-ss5c8
  Normal  SuccessfulCreate  9m31s             replicaset-controller  Created pod: multitool-bshs2
  Normal  SuccessfulDelete  53s               replicaset-controller  Deleted pod: multitool-ss5c8
  Normal  SuccessfulDelete  53s               replicaset-controller  Deleted pod: multitool-bshs2
  Normal  SuccessfulDelete  6s (x2 over 40s)  replicaset-controller  Deleted pod: multitool-rogue
```

From the last line, we can see that the replicaset deleted our rogue pod, to reconsile our desired state of 3 pods, even though the pod wasn't created by the replicaset.


# Service discovery in PV and PVC
In the file ./pvc/pv.yaml you will find a manifest for a Persistent Volume 
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: multitool
  labels:
    volume-type: multitool
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/"
```

As we can see, it has a label key `volume-type` with the value `multitool`. This is our way to label it so that we can pair it with our PVC. Apply the yaml manifest

```
kubectl apply -f pv.yaml
```

Now we need to apply our PVC and match it to our PV. We dont want it to just pick any random PV that matches the criterias like size and accessMode. So we use a selector to specify which PV we want. This could be a pool of PV with the same label, but in this case it's just the above PV.

Examine the file ./pvc/pvc.yaml
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: multitool
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  selector:
    matchLabels:
      volume-type: multitool
```

The last part of the PVC manifest specifies a selector to match the label of PV to bind with the label key `volume-type` and value `multitool`. Than means that it wont just pick any PV to bind, but only ones with this label.

Apply the PVC 
```
kubectl apply -f pvc.yaml
```

and see that it binds the correct PV with the script `check.sh` or
```
kubectl get pv -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,CLAIM:.spec.claimRef.name -l volume-type=multitool

NAME        STATUS   CLAIM
multitool   Bound    multitool
```

We see that the PV is bound by the PVC (claim) multitool. Awesome.


# DNS lookup
While the core service discovery is happening in Kubernetes, most people will not interact with it nor define it as the above, but instead as the the result of DNS being able to resolve pods ip. The very nature of the DNS is simply to resolve a name to an ip. In this case an cluster DNS to a cluster ip of an service. The ip of a service is static, but the endpoints behind it is very dynamic. The discovery part is done by the service, not the DNS. 

Still we would like to show how it works, just to complete the topic.

We have a service called `multitool` in the namespace `default`. We also have a replicaset called `multitool` with a relationship with three pods. If we exec into on of these pods and curl it's own service we should get round-robin'd to the three pods.

```
kubectl exec -it multitool-82c25 bash
curl multitool
Praqma Network MultiTool (with NGINX) - multitool-82c25 - 10.42.2.50/24
curl multitool
Praqma Network MultiTool (with NGINX) - multitool-glhjz - 10.42.0.43/24
curl multitool
Praqma Network MultiTool (with NGINX) - multitool-jlgk6 - 10.42.2.50/24

```

We can also use the full name like this and be explicit 
```
curl multitool.default.svc.cluster.local
Praqma Network MultiTool (with NGINX) - multitool-jlgk6 - 10.42.1.202/24
```

And easy way to find the full name if in doubt is to use `nslookup [Service name]` 
```
nslookup multitool
Server:		10.43.0.10
Address:	10.43.0.10#53

Name:	multitool.default.svc.cluster.local
Address: 10.43.78.78
```

# Clean up
Always remember to clean up.

```
kubectl delete pvc multitool
kubectl delete pv multitool
kubectl delete service multitool
kubectl delete replicaset multitool
```
